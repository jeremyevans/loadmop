#!/usr/bin/env ruby

require 'bundler'
Bundler.setup
Bundler.require(:default)

require 'pathname'
require 'csv'

class LoadMOPCli < Thor
  include Sequelizer

  desc 'create_vocab_tables', 'Connects to database specified in the .env file and loads the OMOP Vocabulary schema'
  def create_vocab_tables
    Sequel.extension :migration
    Sequel::Migrator.run(db, 'schemas/vocabulary', target: 1)
  end

  desc 'create_vocab_database vocab_file_path', 'Connects to database specified in the .env file and loads the OMOP Vocabulary schema and data into it'
  def create_vocab_database(file_path)
    create_vocab_tables
    fix_vocab_data(file_path)
    fast_load(file_path) || slow_load(file_path)
    create_vocab_indexes
  end

  desc 'fix_vocab_data vocab_file_path', 'The vocab files come with a \'|\' character appended to each line, so we clean this off'
  def fix_vocab_data(file_path)
    Pathname.glob(file_path + '/*.csv') do |vocab_file|
      clean_dir = vocab_file.dirname + 'cleaned'
      clean_dir.mkdir unless clean_dir.exist?
      clean_file = clean_dir + vocab_file.basename
      next if clean_file.exist?
      puts "Cleaning #{vocab_file}"
      system("head -n 1 #{vocab_file} | sed 's/\|$//' | tr '[A-Z]' '[a-z]' >> #{clean_file}")
      system("tail -n +2 #{vocab_file} | sed 's/\|$//' >> #{clean_file}")
    end
  end

  desc 'create_vocab_indexes', 'Loads some indexes into the vocabulary database.  Wait until data is already loaded to ensure loading data is fast'
  def create_vocab_indexes
    Sequel.extension :migration
    Sequel::Migrator.run(db, 'schemas/vocabulary', target: 2)
  end

  desc 'test_connection', 'Makes sure conncetion works'
  def test_connection
    puts db[:vocabulary].count
  end

  private

  def fast_load(file_path)
    case adapter
    when 'postgres'
      fast_load_postgres(file_path)
    when 'sqlite'
      fast_load_sqlite(file_path)
    else
      nil
    end
  end

  def fast_load_postgres(file_path)
    Pathname.glob(Pathname.new(file_path) + 'cleaned' + '*.csv') do |vocab_file|
      table_name = vocab_file.basename('.*').to_s.downcase.to_sym
      puts "Loading #{vocab_file} into #{table_name}"
      headers = headers_for(vocab_file).map(&:to_sym)
      db[table_name].truncate
      db.copy_into(
        table_name,
        format:  :csv,
        columns: headers,
        options: 'header',
        data:    File.read(vocab_file)
      )
    end
    true
  end

  def fast_load_sqlite(file_path)
    return nil if `which sqlite3` =~ /not found/i
    db_file_path = database
    Pathname.glob(Pathname.new(file_path) + 'cleaned' + '*.csv') do |vocab_file|
      File.open('/tmp/sqlite.load', 'w') do |file|
        table_name = vocab_file.basename('.*').to_s.downcase
        puts "Loading #{vocab_file} into #{table_name}"
        file.puts %Q(.echo on)
        file.puts %Q(.log stdout)
        file.puts %Q(DELETE FROM #{table_name};)
        file.puts %Q(.mode csv)
        file.puts ".import /dev/stdin #{table_name}"
      end
      command = "tail -n +2 #{vocab_file} | sqlite3 #{db_file_path} '.read /tmp/sqlite.load'"
      puts command
      system(command)
    end
    true
  end

  def slow_load(file_path)
    Pathname.glob(Pathname.new(file_path) + 'cleaned' + '*.csv') do |vocab_file|
      table_name = vocab_file.basename('.*').to_s.downcase.to_sym
      puts "Loading #{vocab_file} into #{table_name}"
      CSV.open(vocab_file, headers: true) do |csv|
        csv.each_slice(1000) do |rows|
          print '.'
          db[table_name].import(headers_for(vocab_file), rows.map(&:fields))
        end
      end
      puts
    end
  end

  def adapter
    db.sequelizer_options['adapter']
  end

  def database
    db.sequelizer_options['database']
  end

  def headers_for(file_path)
    header_line = File.readlines(file_path).first.downcase
    CSV.parse(header_line).first
  end
end

LoadMOPCli.start(ARGV)
